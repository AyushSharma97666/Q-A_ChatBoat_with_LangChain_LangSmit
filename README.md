# ğŸ“„ Transcript Q&A with LLMs  

This project demonstrates how to build a **Transcript Q&A system** using **Google Cloud Functions, Streamlit, and Large Language Models (LLMs)**.  
It serves as a showcase for integrating **LangChain, LangSmith, and GenAPI** to enable scalable, production-grade LLM applications.  

---

## ğŸš€ Project Objective  
The primary goal of this project is to:  
- Showcase how **LangChain** can be used to structure prompts, manage memory, and orchestrate LLM workflows.  
- Demonstrate **LangSmith** for observability, debugging, and evaluation of LLM interactions.  
- Illustrate the use of **GenAPI** for secure and efficient interaction with LLMs in enterprise applications.  
- Build an **interactive Q&A system** where users can input transcripts and ask questions, with answers generated by **Google Gemini models** hosted on GCP.  

---

## ğŸ› ï¸ Tech Stack  
- **Frontend:** [Streamlit](https://streamlit.io/) â€“ interactive UI for transcript input and Q&A.  
- **Backend:** [Google Cloud Functions](https://cloud.google.com/functions) â€“ serverless function for LLM invocation.  
- **LLMs:** [Google Gemini API](https://ai.google.dev/gemini-api) (e.g., `gemini-pro`, `gemini-1.5-flash`).  
- **Orchestration:** [LangChain](https://www.langchain.com/) â€“ prompt management and chain execution.  
- **Observability:** [LangSmith](https://smith.langchain.com/) â€“ tracing, evaluation, and monitoring of LLM calls.  
- **API Layer:** GenAPI â€“ ensures structured communication with LLMs.  

---

## âš™ï¸ Project Structure  

```bash
.
â”œâ”€â”€ chatbot.py                  # Streamlit chatbot for transcript Q&A
â”œâ”€â”€ chatbot_with_history.py     # Chatbot with session-based chat history
â”œâ”€â”€ RAG_v1.py                   # Chatbot with RAG to answer question with context
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ README.md                   # Project documentation
