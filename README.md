📄 Transcript Q&A with LLMs

This project demonstrates how to build a Transcript Q&A system using Google Cloud Functions, Streamlit, and Large Language Models (LLMs).
It serves as a showcase for integrating LangChain, LangSmith, and GenAPI to enable scalable, production-grade LLM applications.

🚀 Project Objective

The primary goal of this project is to:

Showcase how LangChain can be used to structure prompts, manage memory, and orchestrate LLM workflows.

Demonstrate LangSmith for observability, debugging, and evaluation of LLM interactions.

Illustrate the use of GenAPI for secure and efficient interaction with LLMs in enterprise applications.

Build an interactive Q&A system where users can input transcripts and ask questions, with answers generated by Google Gemini models hosted on GCP.

🛠️ Tech Stack

Frontend: Streamlit
 – interactive UI for transcript input and Q&A.

Backend: Google Cloud Functions
 – serverless function for LLM invocation.

LLMs: Google Gemini API
 (e.g., gemini-pro, gemini-1.5-flash).

Orchestration: LangChain
 – prompt management and chain execution.

Observability: LangSmith
 – tracing, evaluation, and monitoring of LLM calls.

API Layer: GenAPI – ensures structured communication with LLMs.

⚙️ Project Structure
.
├── chatbot.py                  # Streamlit chatbot for transcript Q&A
├── chatbot_with_history.py     # Chatbot with session-based chat history
├── requirements.txt            # Python dependencies
├── README.md                   # Project documentation

📝 Notes

chatbot.py is the basic chatbot implementation for transcript Q&A.

chatbot_with_history.py extends the chatbot by tracking chat history in session state, so users can have conversational continuity.