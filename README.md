# 📄 Transcript Q&A with LLMs  

This project demonstrates how to build a **Transcript Q&A system** using **AWS Lambda, Streamlit, and Large Language Models (LLMs)**.  
It serves as a showcase for integrating **LangChain, LangSmith, and GenAPI** to enable scalable, production-grade LLM applications.  

---

## 🚀 Project Objective  
The primary goal of this project is to:  
- Showcase how **LangChain** can be used to structure prompts, manage memory, and orchestrate LLM workflows.  
- Demonstrate **LangSmith** for observability, debugging, and evaluation of LLM interactions.  
- Illustrate the use of **GenAPI** for secure and efficient interaction with LLMs in enterprise applications.  
- Build an **interactive Q&A system** where users can input transcripts and ask questions, with answers generated by Amazon Bedrock-hosted LLMs.  

---

## 🛠️ Tech Stack  
- **Frontend:** [Streamlit](https://streamlit.io/) – interactive UI for transcript input and Q&A.  
- **Backend:** [AWS Lambda](https://aws.amazon.com/lambda/) – serverless function for LLM invocation.  
- **LLMs:** Amazon Bedrock models (e.g., `amazon.nova-pro-v1:0`, `amazon.nova-micro-v1:0`).  
- **Orchestration:** [LangChain](https://www.langchain.com/) – prompt management and chain execution.  
- **Observability:** [LangSmith](https://smith.langchain.com/) – tracing, evaluation, and monitoring of LLM calls.  
- **API Layer:** GenAPI – ensures structured communication with LLMs.  

---

## ⚙️ Project Structure  

```bash
.
├── app.py              # Streamlit app for transcript Q&A
├── requirements.txt    # Python dependencies
├── README.md           # Project documentation
